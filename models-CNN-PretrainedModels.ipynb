{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zR9OwShOa-0-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.applications import ResNet50, EfficientNetB7, VGG16\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJmavm64bAde"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNiaqwgtbEwJ"
      },
      "outputs": [],
      "source": [
        "#Extracting Data\n",
        "! unzip /content/drive/MyDrive/species_resized_256.zip -d /content/images &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOhTIBLq5scV"
      },
      "outputs": [],
      "source": [
        "#Extracting saved model\n",
        "! unzip /content/drive/MyDrive/model.zip -d /content &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmsdMO9cwsc5"
      },
      "outputs": [],
      "source": [
        "# Set the main data directory\n",
        "data_dir = '/content/images/'\n",
        "\n",
        "# Get a list of all labels (subdirectories)\n",
        "labels = os.listdir(data_dir)\n",
        "\n",
        "# Create train, test, and validation directories\n",
        "train_dir = '/content/train'\n",
        "test_dir = '/content/test'\n",
        "val_dir = '/content/validation'\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "# Loop through each label and split data into train, test, and validation\n",
        "for label in labels:\n",
        "    label_dir = os.path.join(data_dir, label)\n",
        "    all_files = os.listdir(label_dir)\n",
        "\n",
        "    # Split the data into train and test\n",
        "    train_files, test_files = train_test_split(all_files, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Split the test set into test and validation\n",
        "    test_files, val_files = train_test_split(test_files, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Create label-specific directories in train, test, and validation\n",
        "    train_label_dir = os.path.join(train_dir, label)\n",
        "    test_label_dir = os.path.join(test_dir, label)\n",
        "    val_label_dir = os.path.join(val_dir, label)\n",
        "\n",
        "    os.makedirs(train_label_dir, exist_ok=True)\n",
        "    os.makedirs(test_label_dir, exist_ok=True)\n",
        "    os.makedirs(val_label_dir, exist_ok=True)\n",
        "\n",
        "    # Move files to their respective directories\n",
        "    for filename in train_files:\n",
        "        shutil.move(os.path.join(label_dir, filename), os.path.join(train_label_dir, filename))\n",
        "\n",
        "    for filename in test_files:\n",
        "        shutil.move(os.path.join(label_dir, filename), os.path.join(test_label_dir, filename))\n",
        "\n",
        "    for filename in val_files:\n",
        "        shutil.move(os.path.join(label_dir, filename), os.path.join(val_label_dir, filename))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfUgIqLYbSPq",
        "outputId": "a58d47a9-1249-4c80-80b7-d56e8d478e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 28509 images belonging to 10 classes.\n",
            "Found 3562 images belonging to 10 classes.\n",
            "Found 3570 images belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "# Set up ImageDataGenerator for training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    cval=0,\n",
        "    brightness_range=(0.7,1.3)\n",
        ")\n",
        "\n",
        "# Set up ImageDataGenerator for testing and validation data (only rescaling)\n",
        "test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Target size of images\n",
        "target_size = (224, 224)\n",
        "\n",
        "# Generate training data from the directory\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/train',\n",
        "    target_size=target_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Generate testing data from the directory\n",
        "test_generator = test_val_datagen.flow_from_directory(\n",
        "    '/content/test',\n",
        "    target_size=target_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Generate validation data from the directory\n",
        "validation_generator = test_val_datagen.flow_from_directory(\n",
        "    '/content/validation',\n",
        "    target_size=target_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW9V2bT8bqEd"
      },
      "outputs": [],
      "source": [
        "def predict_generator(model,test_generator):\n",
        "\n",
        "  # Get the number of batches in the test generator\n",
        "  num_batches = len(test_generator)\n",
        "\n",
        "  # Initialize lists to store true and predicted labels\n",
        "  true_labels = []\n",
        "  predicted_labels = []\n",
        "\n",
        "  # Loop through each batch in the test generator\n",
        "  for i in range(num_batches):\n",
        "      # Get the batch of data\n",
        "      batch_data, batch_true_labels = test_generator[i]\n",
        "\n",
        "      # Predict the labels for the batch\n",
        "      batch_predictions = model.predict(batch_data)\n",
        "\n",
        "      # Get the predicted class indices for each sample in the batch\n",
        "      batch_predicted_indices = np.argmax(batch_predictions, axis=1)\n",
        "\n",
        "      # Extend the true and predicted label lists\n",
        "      true_labels.extend(np.argmax(batch_true_labels, axis=1))\n",
        "      predicted_labels.extend(batch_predicted_indices)\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "  # Print results\n",
        "  print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "  print(classification_report(true_labels,predicted_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bECapsyYxp9K"
      },
      "outputs": [],
      "source": [
        "# Load the ResNet50 model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model with custom dense layers\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit_generator(train_generator, epochs=10, validation_data=validation_generator, verbose=2)\n",
        "\n",
        "# Make predictions on the test data and prints the results\n",
        "predict_generator(model,test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqjbWRqP8G2Z"
      },
      "outputs": [],
      "source": [
        "# Load the EfficientNetB7 model\n",
        "base_model = EfficientNetB7(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "# Freeze the pre-trained weights\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model with custom dense layers\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024,activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit_generator(train_generator, epochs=10, validation_data=validation_generator, verbose=2)\n",
        "\n",
        "# Make predictions on the test data and prints the results\n",
        "predict_generator(model,test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXYzf7lsuIAJ"
      },
      "outputs": [],
      "source": [
        "# Load the EfficientNetB7 model\n",
        "base_model = VGG16(weights=\"imagenet\",include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Create a new model with custom dense layers\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256,activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "checkpoint_path = \"model_checkpoint.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_path,monitor='val_accuracy',save_best_only=True,mode='max',verbose=1)\n",
        "\n",
        "# Train the model with the callback\n",
        "history = model.fit_generator(train_generator,epochs=10,validation_data=validation_generator,verbose=2,callbacks=[checkpoint])\n",
        "\n",
        "# Unfreeze the pre-trained layers\n",
        "for layer in model.layers:\n",
        "  layer.trainable = True\n",
        "\n",
        "# Compile the model again with a low learning rate to fine-tune the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
        "\n",
        "# Train the model with the callback\n",
        "history = model.fit_generator(train_generator, epochs=5, validation_data=validation_generator, verbose=2,callbacks=[checkpoint])\n",
        "\n",
        "# Make predictions on the test data and print the results\n",
        "predict_generator(model,test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V5bCa2qtZci"
      },
      "outputs": [],
      "source": [
        "# Load the EfficientNetB7 model\n",
        "base_model = VGG16(weights=\"imagenet\",include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Create a new model with custom dense layers\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256,activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
        "\n",
        "# Specify the path to the checkpoint\n",
        "checkpoint_path = \"model_checkpoint.h5\"\n",
        "\n",
        "# Load the model weights from checkpoint_path\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "checkpoint = ModelCheckpoint(checkpoint_path,monitor='val_accuracy',save_best_only=True,mode='max',verbose=1)\n",
        "\n",
        "# Train the model with the callback\n",
        "history = model.fit_generator(train_generator, epochs=5, validation_data=validation_generator, verbose=2,callbacks=[checkpoint])\n",
        "\n",
        "# Make predictions on the test data and print the results\n",
        "predict_generator(model,test_generator)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}